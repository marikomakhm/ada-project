{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who's the biggest tax evader?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load country codes\n",
    "df_country_codes = pd.read_csv('data/countries_codes.csv', low_memory=False).set_index('COUNTRY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "## Load panama papers datasets\n",
    "pp_edges = pd.read_csv('data/panama_papers/panama_papers.edges.csv', low_memory=False)\n",
    "pp_nodes_address = pd.read_csv('data/panama_papers/panama_papers.nodes.address.csv', low_memory=False)\n",
    "pp_nodes_entity = pd.read_csv('data/panama_papers/panama_papers.nodes.entity.csv', low_memory=False)\n",
    "pp_nodes_intermediary = pd.read_csv('data/panama_papers/panama_papers.nodes.intermediary.csv', low_memory=False)\n",
    "pp_nodes_officer = pd.read_csv('data/panama_papers/panama_papers.nodes.officer.csv', low_memory=False)\n",
    "## Load UN datasets\n",
    "un_hdi_components_2014 = pd.read_csv('data/un/hdi_components.csv', low_memory=False)\n",
    "un_gdp_per_capita = pd.read_csv('data/un/gdp_per_capita.csv', low_memory=False)\n",
    "un_gdp_per_capita_ppp = pd.read_csv('data/un/gdp_per_capita_PPP.csv', low_memory=False)\n",
    "## Load world bank datasets\n",
    "wb_gini = pd.read_csv('data/world_bank/gini_index.csv', low_memory=False)\n",
    "wb_income_share_20_per = pd.read_csv('data/world_bank/income_share_20_per.csv', low_memory=False)\n",
    "wb_population_total = pd.read_csv('data/world_bank/population_total.csv', low_memory=False)\n",
    "# TIM\n",
    "wb_co2 = pd.read_excel('data/co2_emissions.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only consider statistics that date from 2000 onwards\n",
    "years_to_drop = list(map(str, np.arange(1960, 2000)))\n",
    "wb_gini = wb_gini.drop(columns=years_to_drop)\n",
    "wb_income_share_20_per = wb_income_share_20_per.drop(columns=years_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select the rightmost value (most recent) for each row\n",
    "gini_values = wb_gini.stack().groupby(level=0).last().reindex(wb_gini.index)\n",
    "\n",
    "# Only select valid values and label other values as NaN\n",
    "wb_gini['Gini'] = pd.to_numeric(gini_values, errors='coerce')\n",
    "\n",
    "# Only select relevant columns\n",
    "wb_gini = wb_gini[['Country Name', 'Country Code', 'Gini']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select the rightmost value (most recent) for each row\n",
    "income_share_20_per_values = wb_income_share_20_per.stack().groupby(level=0).last().reindex(wb_income_share_20_per.index)\n",
    "\n",
    "# Only select valid values and label other values as NaN\n",
    "wb_income_share_20_per['Income Share'] = pd.to_numeric(income_share_20_per_values, errors='coerce')\n",
    "\n",
    "# Only select relevant columns\n",
    "wb_income_share_20_per = wb_income_share_20_per[['Country Name', 'Country Code', 'Income Share']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_hdi_components_2014 = un_hdi_components_2014[un_hdi_components_2014['Human Development Index (HDI)'] != '..']\n",
    "un_hdi_components_2014['Human Development Index (HDI)'] = un_hdi_components_2014['Human Development Index (HDI)'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join UN datasets with country codes DataFrame\n",
    "un_hdi_components_2014 = un_hdi_components_2014.join(df_country_codes, on='Country')\n",
    "un_gdp_per_capita = un_gdp_per_capita.join(df_country_codes, on='Country')\n",
    "un_gdp_per_capita_ppp = un_gdp_per_capita_ppp.join(df_country_codes, on='Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of UN DataFrames\n",
    "un_dfs = [un_hdi_components_2014, un_gdp_per_capita, un_gdp_per_capita_ppp]\n",
    "\n",
    "# Define dictionary containing pairs (country name: ISO country code)\n",
    "countries = dict()\n",
    "\n",
    "for country in pycountry.countries:\n",
    "    countries[country.name] = country.alpha_3  \n",
    "\n",
    "for df in un_dfs:\n",
    "    nan_values = df['CODE'].isna()\n",
    "    input_countries = list(df[nan_values]['Country'].values)\n",
    "        \n",
    "    codes = []\n",
    "    for country in input_countries:\n",
    "        if country in countries:\n",
    "            codes.append(countries.get(country))\n",
    "        else:        \n",
    "            accepted = []\n",
    "            str_country = str(country)\n",
    "            # check if string contains either common or official country name\n",
    "            for p_country in pycountry.countries:\n",
    "                if p_country.name in str_country or (hasattr(p_country, 'common_name') and p_country.common_name in str_country):\n",
    "                    accepted.append(p_country.alpha_3)\n",
    "            if len(accepted) == 1:\n",
    "                codes.append(accepted[0])\n",
    "            else:\n",
    "                codes.append(None)\n",
    "\n",
    "    df.loc[nan_values, 'CODE'] = codes\n",
    "    # Remove rows that were not found\n",
    "    df = df[df['CODE'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pp_references_country = pp_nodes_address.groupby(['country_codes', 'countries']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_population_2014 = wb_population_total[['Country Code', '2014']]\n",
    "occurrence_pop = pp_references_country.merge(wb_population_2014, left_on='country_codes', right_on='Country Code')\n",
    "occurrence_pop['counts_1000'] = 1000 * occurrence_pop['counts'] / occurrence_pop['2014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIM\n",
    "\n",
    "# We select the rightmost value (most recent) for each row\n",
    "wb_co2_values = wb_co2.stack().groupby(level=0).last().reindex(wb_co2.index)\n",
    "\n",
    "# Only select valid values and label other values as NaN\n",
    "wb_co2['CO2 Emissions'] = pd.to_numeric(wb_co2_values, errors='coerce')\n",
    "\n",
    "# Only keep most recent values for each country\n",
    "wb_co2 = wb_co2[['Country Name', 'Country Code', 'CO2 Emissions']]\n",
    "\n",
    "# Remove countries without indicator information\n",
    "wb_co2 = wb_co2.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data analysis and observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Panama Papers and population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence_pop['counts_1000'].plot.hist(title='Distribution of references per 1000 inhabitants', bins=100, logy=True)\n",
    "plt.xlabel('Number of occurrences in Panama Papers per 1000 inhabitants')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_intermediary_country = pp_nodes_intermediary.groupby(['country_codes', 'countries']).size().reset_index(name='counts')\n",
    "pp_intermediary_country = pp_intermediary_country.sort_values('counts', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the distribution using a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ dict(\n",
    "        type = 'choropleth',\n",
    "        locations = pp_intermediary_country['country_codes'],\n",
    "        z = pp_intermediary_country['counts'],\n",
    "        text = pp_intermediary_country['countries'],\n",
    "        colorscale = [[0,\"rgb(5, 10, 172)\"],[0.35,\"rgb(40, 60, 190)\"],[0.5,\"rgb(70, 100, 245)\"],\\\n",
    "            [0.6,\"rgb(90, 120, 245)\"],[0.7,\"rgb(106, 137, 247)\"],[1,\"rgb(220, 220, 220)\"]],\n",
    "        autocolorscale = False,\n",
    "        reversescale = True,\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(180,180,180)',\n",
    "                width = .3\n",
    "            ) ),\n",
    "        colorbar = dict(\n",
    "            autotick = False,\n",
    "            tickprefix = '',\n",
    "            title = 'Number of references'),\n",
    "      ) ]\n",
    "\n",
    "\"\"\"\n",
    "layout = {\n",
    "  \"geo\": {\n",
    "    \"coastlinewidth\": 2, \n",
    "    \"countrycolor\": \"rgb(204, 204, 204)\", \n",
    "    \"lakecolor\": \"rgb(255, 255, 255)\", \n",
    "    \"landcolor\": \"rgb(204, 204, 204)\", \n",
    "    \"lataxis\": {\n",
    "      \"dtick\": 10, \n",
    "      \"range\": [20, 60], \n",
    "      \"showgrid\": True\n",
    "    }, \n",
    "    \"lonaxis\": {\n",
    "      \"dtick\": 20, \n",
    "      \"range\": [-100, 20], \n",
    "      \"showgrid\": True\n",
    "    }, \n",
    "    \"projection\": {\"type\": \"equirectangular\"}, \n",
    "    \"resolution\": 50, \n",
    "    \"showlakes\": True, \n",
    "    \"showland\": False\n",
    "  }, \n",
    "  \"showlegend\": False, \n",
    "  \"title\": \"Seoul to Hong Kong Great Circle\"\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "layout = dict(\n",
    "    title = 'References in Panama Papers',\n",
    "    geo = dict(\n",
    "        showcountries = True,\n",
    "        countrycolor = \"rgb(217, 217, 217)\",\n",
    "        showframe = False,\n",
    "        resolution=110,\n",
    "        showcoastlines = False,\n",
    "        bgcolor = 'rgba(255, 255, 255, 0.0)',\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig = dict( data=data, layout=layout )\n",
    "\n",
    "iplot( fig, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = pp_intermediary_country['counts'].min()\n",
    "max_count = pp_intermediary_country['counts'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstOrDefault(values, default):\n",
    "    if values is None or len(values) == 0:\n",
    "        return default\n",
    "    return values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Milestone 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the tables that most of the countries involved in the Panama Papers affair are small islands, which unfortunately are not displayed by the `Plotly` library. For the next milestone, we will fix that issue either by finding a solution that still works with `Plotly` or by using a different library, such as `folium`.\n",
    "\n",
    "So far, we have made insightful observations that match the reports found in the media, particularly about which countries were most involved in this affair.\n",
    "\n",
    "For the next milestone, we will further investigate the links between the countries, and try to understand the correlation of socio-economic factors with the locations of entities, officers and intermediaries involved in Panama Papers. More specifically, we intend to:\n",
    "- Find which socio-economic factors are correlated with the results we found so far, and how they are correlated\n",
    "- Display the links between the countries using a graph similar to the one found [here](https://plot.ly/python/lines-on-maps/)\n",
    "- Fix issues with certain countries (particularly small islands) not being displayed in the graph\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_edges_parsed = pp_edges[['START_ID', 'TYPE', 'END_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_nodes_intermediary_parsed = pp_nodes_intermediary[['node_id', 'country_codes', 'countries']]\n",
    "pp_nodes_entity_parsed = pp_nodes_entity[['node_id', 'country_codes', 'countries']]\n",
    "pp_nodes_officer_parsed = pp_nodes_officer[['node_id', 'country_codes', 'countries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_nodes = pp_nodes_entity_parsed.append(pp_nodes_intermediary_parsed).append(pp_nodes_officer_parsed)\n",
    "pp_nodes = pp_nodes.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_edges_countries = pp_nodes.merge(pp_edges_parsed, left_on='node_id', right_on='START_ID')\n",
    "pp_edges_countries = pp_edges_countries.rename(columns={'node_id': 'id_origin', 'country_codes': 'cc_origin', \n",
    "                                                        'countries': 'c_origin'})\n",
    "pp_edges_countries = pp_edges_countries.merge(pp_nodes, left_on='END_ID', right_on='node_id')\n",
    "pp_edges_countries = pp_edges_countries.rename(columns={'node_id': 'id_dest', 'country_codes': 'cc_dest', \n",
    "                                                        'countries': 'c_dest'})\n",
    "pp_edges_countries = pp_edges_countries.drop(columns=['id_origin', 'id_dest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to find a correlation between the amount of involvement in Panama Papers of the three different types of node and different social economic factors. We considered several different socio-economic factors, but ultimately we decided to focus on:\n",
    "- Human Development Index\n",
    "- GDP per capita\n",
    "- Gini coefficient (measures inequality)\n",
    "- Income share of the 20% richest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple function to count the number of occurrences of the various entity in the Panama Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurence(pp_data):\n",
    "    return pp_data.groupby(['country_codes', 'countries']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use this function to count the number of occurrences of each country in the scandal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_references_officier = count_occurence(pp_nodes_officer)\n",
    "pp_references_officier.sort_values('counts', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_references_entity = count_occurence(pp_nodes_entity)\n",
    "pp_references_entity.sort_values('counts', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_references_intermediary = count_occurence(pp_nodes_intermediary)\n",
    "pp_references_intermediary.sort_values('counts', ascending=False).head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, most of the countries with high presence in the panama papers are either countries known to have non strict fiscal laws either country with a very high population. We will later normalize by the population of each country to delete the bias that comes from the population size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be also interested by the total number of occurrences in the Panama papers regardless of the the type of node. Let's compute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_references_total = pp_references_intermediary.merge(pp_references_entity, on='country_codes')\n",
    "pp_references_total = pp_references_total.merge(pp_references_officier, on='country_codes')\n",
    "pp_references_total['counts'] = pp_references_total['counts'] + pp_references_total['counts_x'] + pp_references_total['counts_y']\n",
    "pp_references_total = pp_references_total[['country_codes', 'countries', 'counts']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_references_total.sort_values('counts', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the different datasets in which we will try to find a correlation with the social factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_types = [pp_references_entity, pp_references_intermediary, pp_references_officier, pp_references_total]\n",
    "nodes_types_desc = ['Entity references', 'Intermediary references', 'Officier references', 'Total references']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, the number of occurrences is biased by countries that have a high population. To remove this bias, we want to normalize the counts of occurrences by the population of each country.\n",
    "The function that we define bellow simply does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_count(df):\n",
    "    merged = df.merge(wb_population_2014, left_on='country_codes', right_on='Country Code')[['country_codes', 'countries', 'counts', '2014']]\n",
    "    merged['counts_normalized'] = merged['counts'] / merged['2014']\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing every dataset\n",
    "nodes_types_normalized = list(map(normalize_count, nodes_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GDP\n",
    "The GDP per capita is the value of all the goods and services produced by a country in one year. It therefore represents approximately how rich a country is. We chose to use this indicator because we thought that richer people might have more incentive to try to evade the tax system. We will go deeper in this analyze later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start by selecting only the GDP per capita of 2015 (year of the affair). We will then merge the GDP dataset with all the datasets of the various type of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_2015 = un_gdp_per_capita[un_gdp_per_capita['Year'] == 2015][['CODE', 'Value']]\n",
    "gdp_data = [x.merge(gdp_2015, left_on='country_codes', right_on='CODE') for x in nodes_types_normalized]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function that will create a scatter plot between an economic index and all the types of nodes. This function will also compute the Pearson and Spearman correlation coefficient and give us the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_entity(data, x_name, y_name, data_name, index_activated, title, x_label, y_label, save_name):\n",
    "    traces = []\n",
    "    i = 0\n",
    "    for d in data:\n",
    "        # Create a trace\n",
    "        trace = go.Scatter(\n",
    "            name = data_name[i],\n",
    "            x = d[x_name],\n",
    "            y = d[y_name],\n",
    "            mode = 'markers',\n",
    "            text = d['countries'],\n",
    "            visible = 'legendonly' if i != index_activated else True\n",
    "        )\n",
    "        traces.append(trace)\n",
    "        \n",
    "        corr, p = stats.pearsonr(d[x_name], d[y_name])\n",
    "        print(data_name[i])\n",
    "        print('\\tpearson : ', corr, '; p-val: ', p)\n",
    "        corr, p = stats.spearmanr(d[x_name], d[y_name])\n",
    "        print('\\tspearman : ', corr, '; p-val: ', p)\n",
    "        i+=1\n",
    "    data = traces\n",
    "    \n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        xaxis=dict(\n",
    "            title=x_label\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=y_label\n",
    "        ),\n",
    "        hovermode = 'closest'\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    file_path = '../TrovatelliT.github.io/ressources/' + save_name\n",
    "    iplot(fig)\n",
    "    plot(fig, filename=file_path, auto_open=False) # Also save the plot for the website\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the scatter plot showing the correlation between the GDP per capita and the number of total occurrences normalized in the scandal. As said earlier, we will only consider the number of occurrences normalized since countries with high populations will render the number of counts more biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_all_entity(gdp_data, 'counts_normalized', 'Value', nodes_types_desc, 3, 'GDP per capita vs number of occurrences in the scandal', 'Number of occurrences in the Panama Papers per inhabitant', 'GDP per capita', 'scatter_gdp_count.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see most there are a few outliers. Most of them are fiscal paradise that have a high number of occurrences and a low number of population (British Virgin Islands, Luxembourg, Bermuda, etc.). As we're plotting the number of occurrences normalized, we would expect such countries to stand out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem we'll only keep points $x$ that respect these conditions:\n",
    "\n",
    "$$q_{.25} - 1.5 \\cdot \\text{IQR} \\leq x \\leq q_{.75} + 1.5 \\cdot \\text{IQR}$$\n",
    "\n",
    "where : \n",
    "\n",
    "- $\\text{IQR}$ is the interquartile range defined by: $q_{.75} - q_{.25}$\n",
    "- $q_t$ is the t-quantile of this distribution\n",
    "\n",
    "From now on, we'll always remove the outliers of every plots\n",
    "\n",
    "We'll also compute the p-value.\n",
    "Our null hypothesis $H_0$ is that the two variables are uncorrelated. We can reject $H_0$ when the p-value is high enough. We'll fix our threshold $\\alpha = .05$. So if a p-value is higher than $.05$ we can reject the fact that the two variables are uncorrelated.\n",
    "\n",
    "Below we'll only analyze variables that have a significant correlation.\n",
    "\n",
    "\n",
    "The function below implement that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data, field1, field2):\n",
    "    Q1_1 = data[field1].quantile(0.25)\n",
    "    Q3_1 = data[field1].quantile(0.75)\n",
    "    IQR_1 = Q3_1 - Q1_1\n",
    "    \n",
    "    Q1_2 = data[field2].quantile(0.25)\n",
    "    Q3_2 = data[field2].quantile(0.75)\n",
    "    IQR_2 = Q3_2 - Q1_2\n",
    "\n",
    "    filter1 = (data[field1] < Q3_1 + 1.5 * IQR_1) & (data[field1] > Q1_1 - 1.5 * IQR_1)\n",
    "    filter2 = (data[field2] < Q3_2 + 1.5 * IQR_2) & (data[field2] > Q1_2 - 1.5 * IQR_2)\n",
    "    return data[filter1 & filter2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now plot the same plot without the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(map(lambda d: remove_outliers(d, 'Value', 'counts_normalized'), gdp_data))\n",
    "plot_all_entity(data, 'counts_normalized', 'Value', nodes_types_desc, 3, 'GDP per capita vs number of occurrences in the scandal without outliers', 'Number of occurrences in the Panama Papers per inhabitant', 'GDP per capita', 'scatter_gdp_count_outlier.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if on this graph the correlation are smaller, it is can better see the pattern that the number are suggesting. \n",
    "We can see that for the officers the correlation coefficients are very high. The p-values are very small and allow us to reject the fact that the references of officer and the GDP is uncorrelated.\n",
    "\n",
    "\n",
    "We observe that countries where the GDP is higher appear more often in the Panama Papers. Again, this is what we would intuitively predict.\n",
    "Richer countries have usually better infrastructure and therefore have a system to ensure that the taxes are actually collected.\n",
    "This provides a much larger incentive for people to try to evade this systems by any means they can, which leads to scandals such as this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gini coefficient measures the inequality in the distribution of wealth in a country (lower coefficient means lower inequality). The most equal equal society is when every person get the same income (When gini = 0). Our initial assumption was that countries in which there is high levels of income inequality would be more heavily involved in this affair, as this is generally heavily linked with corruption and tax evasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_nan = wb_gini.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same as for the GDP. We join the dataset with the Gini coefficients and with the number of occurrences in the scandal. We then only show the plot without the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_data = [x[x['counts'] > 0].merge(gini_nan, left_on='country_codes', right_on='Country Code') for x in nodes_types_normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = list(map(lambda d: remove_outliers(d, 'Gini', 'counts_normalized'), gini_data))\n",
    "plot_all_entity(data, 'counts_normalized', 'Gini', nodes_types_desc, 3, 'Gini coefficient vs number of occurrenes in the scandal without outliers', 'Number of occurrences per inhabitant', 'Gini Index', 'scatter_gini_count_outlier.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the table above, the correlation between the number of occurrence and the Gini coefficient is not really relevant as the high p-value suggests.\n",
    "\n",
    "We can then not reject the hypothesis $H_0$ that the two variables are uncorrelated.\n",
    "\n",
    "Which is surprising !\n",
    "Our initial assumption was that countries in which there is high levels of income inequality would be more heavily involved in this affair. As the tax evasion are higher in those countries, the redistribution of wealth is reduced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income held by top 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This index also measures inequality but it quantifies it differently by expressing the share of wealth held to the top 20% richest. We would expect similar results as for the Gini coefficient since both indices are measuring inequality.\n",
    "\n",
    "We'll again show the plot without the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_income_share_nan = wb_income_share_20_per.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_share_data = [x.merge(wb_income_share_nan, left_on='country_codes', right_on='Country Code') for x in nodes_types_normalized]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = list(map(lambda d: remove_outliers(d, 'counts_normalized', 'Income Share'), income_share_data))\n",
    "plot_all_entity(data, 'counts_normalized', 'Income Share', nodes_types_desc, 3, 'Wealth held by 20% richest vs number of occurrences in the scandal', 'Number of occurences per inhabitant', 'Wealth held by 20% richest [%]', 'scatter_20_count_outliers.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the correlation coefficients are low and the p-values are very high. We can again not reject $H_0$.\n",
    "There is almost no correlation which is coherent with what we observed before for the Gini coefficient.\n",
    "\n",
    "Unfortunately, we do not get anymore insight by using this metric when studying the involvement of a country in Panama Papers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDI index try to represent how well a country is developed by using the lifespan, the education level and the GDP per capita of an average citizen to compute the index.  \n",
    "We now observe if there is a correlation between the HDI and the number of occurrence in this affair.\n",
    "\n",
    "As we've done so far, we will show the scatter plot of the HDI vs the number of occurrences in the Panama Papers without the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdi_data = [x.merge(un_hdi_components_2014, left_on='country_codes', right_on='CODE') for x in nodes_types_normalized]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(map(lambda d: remove_outliers(d, 'Human Development Index (HDI)', 'counts_normalized'), hdi_data))\n",
    "plot_all_entity(data, 'counts_normalized', 'Human Development Index (HDI)', nodes_types_desc, 3, 'HDI vs number of occurrences in the scandal', 'Number of occurences per inhabitant', 'HDI Index', 'scatter_hdi_count_outlier.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value suggest that we can reject $H_0$, therefore the variables are correlated. This is map were we can best visually see the correlation. It seems that for an $\\text{HDI} \\geq 0.65$ there is a big increase in the number of occurrences in the Panama Papers.\n",
    "\n",
    "There seems to be a correlation as the plot and the Spearman coefficient suggest.\n",
    "One of the reasons could be that a country with higher HDI have generally a better infrastructures to ensure that the tax are paid. Additionally, a country with higher HDI is home to richer people since the GDP per capita is also taken into account in the calculation of the HDI. Or maybe it is the case because richer people like to live in developed country? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we studied the impact of socioeconomic factor in the way countries are involved in the Panama papers. We saw that we couldn't show that the degree of inequality within a country influenced how tied to this affair a country was. However we saw that there is a medium a correlation with the GDP per capita and with the HDI as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
